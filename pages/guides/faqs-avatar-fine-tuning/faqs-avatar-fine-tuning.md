# FAQs: Avatar Fine-tuning

## üìù Getting Started

This guide will cover common FAQs we‚Äôve seen around getting the best results after fine-tuning your models

In addition to using these FAQs, see how to prompt correctly with the following guide:

[Stellar Prompts for AI Avatars](/guides/stellar-prompts-for-ai-avatars)

## üôã‚Äç‚ôÇÔ∏è¬†FAQs

### How many images should I ask for from my users?

We recommend 3-8 high quality image samples for training where the subject is directly in frame:

For best results, resize your image samples so that each sample is 512x512 (you can a website like [https://www.birme.net/](https://www.birme.net/) to do so).

Example:

![image](./images/faqs_01.avif)

Note ‚Äî more images is not always better as it can cause the model to overfit, causing results to look too much like the original provided samples.

Example of overfitting:

![image](./images/faqs_02.avif)

### How can I detect good image samples?

For best results, we recommend building an image cropper into your UX so that each sample is 512x512px (or at a consistent aspect ratio that corresponds to the aspect ratio of your expected output).

We also recommend the that faces are directly in view and not too far away. Try to avoid shadows, glasses, hats and other face accessories.

Here are examples of good and bad image samples from Lensa AI.

![image](./images/faqs_03.avif)

To detect is the face is in view, we recommend this api:

[Face-Detection Algorithms for Image Transformations | Cloudinary](https://cloudinary.com/documentation/face_detection_based_transformations)

### What prompts should I use?

See a directory of good prompts here:

[Stellar Prompts for AI Avatars](/guides/stellar-prompts-for-ai-avatars)

Another helpful resource for prompting is CLIP Interrogator, which lets you upload an image and get the expected prompt:

[CLIP Interrogator - a Hugging Face Space by pharma](https://huggingface.co/spaces/pharma/CLIP-Interrogator)

### Should I use restore faces?

If you are working with faces that have already been generated by a tool like DALL-E or VQGAN, we recommend using the "restore faces" option to ensure the highest quality results. However, if you are working with raw images, this option is not necessary.

### What should I do with seeds?

For seeds, we recommend using random seeds to generate new outputs. This can help diversify the results and prevent the model from overfitting to a specific input. Experiment with different seeds to see what kind of outputs you can generate.

### Can the model trained for the customer be reused or retrained?

Yes, once trained, a model can be reused with different prompts to generate different outputs. It can also be retrained on new sample images.

### Can I download the CKPT file?

Currently we don't support downloading the model after fine-tuning. However, you can continue to use the fine-tuned model in our platform to generate new outputs as needed.

### How should I use negative prompts?

Negative prompts are terms that you want the generated image to omit or avoid. We recommend experimenting with the ‚ÄòAuto Suggest‚Äô to see best practices.

![image](./images/faqs_04.avif)

## üöÄ Wrapping Up

That's all for this guide!

Try it out for yourself and let us know if you have any questions. For additional support please join our [discord](https://discord.com/invite/NCAKTUayPK).